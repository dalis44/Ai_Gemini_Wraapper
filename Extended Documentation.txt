======parse_arguments() functon=====
This function is responsible for handling command-line arguments when the script run.
but firstly apikey.env is read in order to load any environment variables (like API keys) into the program 
- this ensures your script has access to secrets without hardcoding them.

We will use as well --verbose flag, that means "show more detailed output"
these "more details" include number of tokens or what function calls the AI model makes , for example, without it we will see just final output.

After skips the first element (sys.argv[0] is always the script name) and filters out any arguments starting with -- (flags).
what remains is treated as the user’s prompt.
Finally we build  the user prompt: user_prompt = " ".join(args)



======build_message() functon=====
1.In order to communicate with our LLM we need to wrap our prompt to a message object

Why do we need the message object?

Because the LLM API needs to know:

who is speaking (user, assistant, system)

what the content is

optional: images, tool calls, formatting, etc.

So the model receives a structured conversation instead of raw text.
- Wraps the user’s prompt into a structured format:  types.Content
- role="user" : indicates this message comes from the user.
- parts=[types.Part(text=user_prompt)] : the actual text content of the prompt.
:::You use role="user" when sending the prompt
:::You use role="tool" when sending tool responses back to the AI
:::You use role="assistant" only in AI responses


2. Return the structured conversation
- The result is a list of messages, ready to be sent to the AI model.
