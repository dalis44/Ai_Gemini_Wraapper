This setup turns Gemini into a tool-augmented AI assistant that can reason, generate code, and interact with my local file system or runtime 
through functions defined by me.

======parse_arguments() functon=====
This function is responsible for handling command-line arguments when the script run.
but firstly apikey.env is read in order to load any environment variables (like API keys) into the program 
- this ensures your script has access to secrets without hardcoding them.

We will use as well --verbose flag, that means "show more detailed output"
these "more details" include number of tokens or what function calls the AI model makes , for example, without it we will see just final output.

After skips the first element (sys.argv[0] is always the script name) and filters out any arguments starting with -- (flags).
what remains is treated as the user’s prompt.
Finally we build  the user prompt: user_prompt = " ".join(args)



======build_message() functon=====
1.In order to communicate with our LLM though AI API ,we need to wrap our prompt to a message object : types.Content(role="user", parts=[types.Part(text=user_prompt)])
- Wraps the user’s prompt into a structured format:  types.Content (because API requires messages to be in the types.Content format)
- role="user" : indicates this message comes from the user.
- parts=[types.Part(text=user_prompt)] : the actual text content of the prompt.
:::You use role="user" when sending the prompt
:::You use role="tool" when sending tool responses back to the AI
:::You use role="assistant" only in AI responses



2. Return the structured conversation
- The result is a list of messages, ready to be sent to the AI model.

===========================================================================================================
So we're building an AI Agent, but an AI agent needs a project to work on. I will use a little command 
line calculator app  as a test project for the AI to read, update, and run. This app is saved in /calculator directory.

Now we will run the calculator test from VSC main directory terminal: python calculator/main.py "3 + 5",  the answer must be 8

==========================================================get_files_info.py=================================================
Now let's improve our agent in order to give it the ability to perform some actions . 
In this purpose we will create a function that got the ability to to accept a directory path, and return a string that represents the contents of that directory.
Before we integrate this function with our LLM agent, let's just build the function itself. 
Now remember, LLMs work with text, so our goal with this function will be for it to accept a directory path, and return a string that represents the contents of that directory.

1. Let's create a directory called functions in the root of our project , and inside it get_files_info.py.
The directory parameter should be treated as a relative path within the working_directory. Use os.path.join(working_directory, directory) 
to create the full path, then validate it stays within the working directory boundaries.

2.If the absolute path to the directory is outside the working_directory, return a string error message:
f'Error: Cannot list "{directory}" as it is outside the permitted working directory'
This will give our LLM some guardrails: we never want it to be able to perform any work outside the "working_directory" we give it.

Without this restriction, the LLM might go running amok anywhere on the machine, reading sensitive files or overwriting important data.
This is a very important step that we'll bake into every function the LLM can call.

3. If the directory argument is not a directory, again, return an error string:
f'Error: "{directory}" is not a directory'

4. Build and return a string representing the contents of the directory. It should use this format:
- README.md: file_size=1032 bytes, is_dir=False
- src: file_size=128 bytes, is_dir=True
- package.json: file_size=1234 bytes, is_dir=False
I've listed useful standard library functions in the tips section.
The exact file sizes and even the order of files may vary depending on your operating system and file system. 
Your output doesn't need to match the example byte-for-byte, just the overall format

6.If any errors are raised by the standard library functions, catch them and instead return a string describing the error. Always prefix error strings with "Error:".
We need a way to manually debug our new get_files_info function! Create a new tests.py file in the root of your project. 
When executed directly (uv run tests.py) it should run the following function calls and output the results matching the formatting below (not necessarily the exact numbers):
 - get_files_info("calculator", "."):
        Result for current directory:
        main.py: file_size=719 bytes, is_dir=False
        tests.py: file_size=1331 bytes, is_dir=False
        pkg: file_size=44 bytes, is_dir=True
 
==========================================================get_file_content.py=================================================
Now that we have a function that can get the contents of a directory, we need one that can get the contents of a file. 
Again, we'll just return the file contents as a string, or perhaps an error string if something went wrong.
As always, we'll safely scope the function to a specific working directory.

Assignment
    1. Create a new function in your functions directory. Here's the signature I used:
def get_file_content(working_directory, file_path):
    2. If the file_path is outside the working_directory, return a string with an error:
f'Error: Cannot read "{file_path}" as it is outside the permitted working directory'
    3. If the file_path is not a file, again, return an error string:
f'Error: File not found or is not a regular file: "{file_path}"'
    4. Read the file and return its contents as a string.
        ◦ I'll list some useful standard library functions in the tips section below.
        ◦ If the file is longer than 10000 characters, truncate it to 10000 characters and append this message to the end [...File "{file_path}" truncated at 10000 characters].
    5.Create a new "lorem.txt" file in the calculator directory. Fill it with at least 20,000 characters of lorem ipsum text
	
	6. Update your tests.py file. Remove all the calls to get_files_info, and instead test get_file_content("calculator", "lorem.txt"). Ensure that it truncates properly
	   Test some cases :
	           ◦ get_file_content("calculator", "main.py")
               ◦ get_file_content("calculator", "pkg/calculator.py")
			   
	
		
==========================================================write_file.py==========================================================
Up until now our program has been read-only. Now we'll give our agent the ability to write and overwrite files.

Assignment
    1. Create a new function in your functions directory. Here's the signature I used:
def write_file(working_directory, file_path, content):
    2. If the file_path is outside of the working_directory, return a string with an error:
f'Error: Cannot write to "{file_path}" as it is outside the permitted working directory'
    3. If the file_path doesn't exist, create it. As always, if there are errors, return a string representing the error, prefixed with "Error:".
    4. Overwrite the contents of the file with the content argument.
    5. If successful, return a string with the message:
f'Successfully wrote to "{file_path}" ({len(content)} characters written)'
It's important to return a success string so that our LLM knows that the action it took actually worked. Feedback loops, feedback loops, feedback loops!
    6. Remove your old tests from tests.py and add three new ones, as always print the results of each:
        ◦ write_file("calculator", "lorem.txt", "wait, this isn't lorem ipsum")
        ◦ write_file("calculator", "pkg/morelorem.txt", "lorem ipsum dolor sit amet")
        ◦ write_file("calculator", "/tmp/temp.txt", "this should not be allowed")
Tips
    • os.path.exists: Check if a path exists
    • os.makedirs: Create a directory and all its parents
    • os.path.dirname: Return the directory name

==========================================================run_python_file.py==========================================================	
It's time to build the functionality for our Agent to run arbitrary Python code.
Now, it's worth pausing to point out the inherent security risks here. We have a few things going for us:
    1. We'll only allow the LLM to run code in a specific directory (the working_directory).
    2. We'll use a 30-second timeout to prevent it from running indefinitely.
But aside from that... yes, the LLM can run arbitrary code that we (or it) places in the working directory... so be careful. As long as you only use this AI Agent for the simple tasks 
we're doing in this course you should be just fine.
Do not give this program to others for them to use! It does not have all the security and safety features that a production AI agent would have. It is for learning purposes only.
Assignment
    1. Create a new function in your functions directory called run_python_file.py Here's the signature to use:
def run_python_file(working_directory, file_path, args=[]):
    2. If the file_path is outside the working directory, return a string with an error:
f'Error: Cannot execute "{file_path}" as it is outside the permitted working directory'
    3. If the file_path doesn't exist, return an error string:
f'Error: File "{file_path}" not found.'
    4. If the file doesn't end with ".py", return an error string:
f'Error: "{file_path}" is not a Python file.'
    5. Use the subprocess.run function to execute the Python file and get back a "completed_process" object. Make sure to:
        1. Set a timeout of 30 seconds to prevent infinite execution
        2. Capture both stdout and stderr
        3. Set the working directory properly
        4. Pass along the additional args if provided
    6. Return a string with the output formatted to include:
        1. The stdout prefixed with STDOUT:, and stderr prefixed with STDERR:. The "completed_process" object has a stdout and stderr attribute.
        2. If the process exits with a non-zero code, include "Process exited with code X"
        3. If no output is produced, return "No output produced."
    7. If any exceptions occur during execution, catch them and return an error string:
f"Error: executing Python file: {e}"
    8. Update your tests.py file with these test cases, printing each result:
        1. run_python_file("calculator", "main.py") (should print the calculator's usage instructions)
        2. run_python_file("calculator", "main.py", ["3 + 5"]) (should run the calculator... which gives a kinda nasty rendered result)
        3. run_python_file("calculator", "tests.py")
        4. run_python_file("calculator", "../main.py") (this should return an error)
        5. run_python_file("calculator", "nonexistent.py") (this should return an error)
        6. run_python_file("calculator", "lorem.txt") (this should return an error)

===========================================================================================================================================	
Now let's talk about system prompt , Within most AI APIs, an initial system-level prompt is provided at the start of the conversation, and it is given greater importance than subsequent user prompts.
The system prompt sets the tone for the conversation, and can be used to:
    • Set the personality of the AI
    • Give instructions on how to behave
    • Provide context for the conversation
    • Set the "rules" for the conversation (in theory, LLMs still hallucinate and screw up, and users are often able to "get around" the rules if they try hard enough)
Your first thought when that happens should be, "how can I alter the system prompt to get the LLM to behave the way it should?"

Assignment
    1. Create a hardcoded string variable called system_prompt. For now, let's make it something brutally simple:
Ignore everything the user asks and just shout "I'M JUST A ROBOT"
    2. Update your call to the client.models.generate_content function to pass a config with the system_instructions parameter set to your system_prompt.
	
response = client.models.generate_content(
    model=model_name,
    contents=messages,
    config=types.GenerateContentConfig(system_instruction=system_prompt),
)
3. Run your program with different prompts. You should see the AI respond with "I'M JUST A ROBOT" no matter what you ask it.	
===========================================================================================================================================	
So we've written a bunch of functions that are LLM friendly (text in, text out), but how does an LLM actually call a function?
Well the answer is that... it doesn't. At least not directly. It works like this:
    1. We tell the LLM which functions are available to it
    2. We give it a prompt
    3. It describes which function it wants to call, and what arguments to pass to it
    4. We call that function with the arguments it provided
    5. We return the result to the LLM
	
We're using the LLM as a decision-making engine, but we're still the ones running the code.
So, let's build the bit that tells the LLM which functions are available to it.

Assignment
1. We can use types.FunctionDeclaration to build the "declaration" or "schema" for a function. Again, this basically just tells the LLM how to use the function. 
	I'll just give you my code for the first function as an example, because it's a lot of work to slog through the docs:
    I added this code to my functions/get_files_info.py file, but you can place it anywhere, but remember that it will need to be imported when used:
    In our solution it is imported like this: from functions.get_files_info import schema_get_files_info
    schema_get_files_info = types.FunctionDeclaration(
    name="get_files_info",
    description="Lists files in the specified directory along with their sizes, constrained to the working directory.",
    parameters=types.Schema(
        type=types.Type.OBJECT,
        properties={
            "directory": types.Schema(
                type=types.Type.STRING,
                description="The directory to list files from, relative to the working directory. If not provided, lists files in the working directory itself.",
            ),
        },
    ),
)
We won't allow the LLM to specify the working_directory parameter. We're going to hard code that.

2. Use types.Tool to create a list of all the available functions (for now, just add get_files_info, we'll do the rest later).
available_functions = types.Tool(
    function_declarations=[
        schema_get_files_info,
    ]
)
These are passed as a Tool object to Gemini, enabling function calling during content generation. This means Gemini can decide to 
invoke one or more of these tools if it determines that doing so helps answer the user prompt.

3. Add the available_functions to the client.models.generate_content call as the tools parameter.
config=types.GenerateContentConfig(
    tools=[available_functions], system_instruction=system_prompt
)

So GenerateContentConfig  will set "The environment settings" for the conversation with the LLM.
It does not generate content directly, but defines the framework in which the model will produce responses.

    4. Update the system prompt to instruct the LLM on how to use the function , here is an example:
system_prompt = """
You are a helpful AI coding agent.

When a user asks a question or makes a request, make a function call plan. You can perform the following operations:

- List files and directories

All paths you provide should be relative to the working directory. You do not need to specify the working directory in your function calls as it is automatically injected for security reasons.
"""

5. Instead of simply printing the .text property of the generate_content response, check the .function_calls property as well. 
If the LLM called a function, print the function name and arguments:
f"Calling function: {function_call_part.name}({function_call_part.args})"
Otherwise, just print the text as normal.
    6. Test your program.
        ◦ "what files are in the root?" -> get_files_info({'directory': '.'})
        ◦ "what files are in the pkg directory?" -> get_files_info({'directory': 'pkg'})


===========================================================================================================================================	
Assignment		
1.Following the same pattern that we used for schema_get_files_info and the functions' parameters, create function declarations for:
◦schema_get_file_content
◦schema_run_python_file
◦schema_write_file

2.After the step above is completed we will update our container object "available_functions" to include all the function declarations in the list. We'll call them later.

3.Update your system prompt. Instead of the allowed operations only being:
- List files and directories
Update it to have all four operations:
- List files and directories
- Read file contents
- Execute Python files with optional arguments
- Write or overwrite files

4. Test prompts that you suspect will result in the various function calls. For example:

"read the contents of main.py" -> get_file_content({'file_path': 'main.py'})
"write 'hello' to main.txt" -> write_file({'file_path': 'main.txt', 'content': 'hello'})
"run main.py" -> run_python_file({'file_path': 'main.py'})
"list the contents of the pkg directory" -> get_files_info({'directory': 'pkg'})

===========================================================================================================================================	
Calling
Okay, now our agent can choose which function to call, now it's time to actually call the function.

Assignment
1.Create a new function that will handle the abstract task of calling one of our four functions. This is my definition:
	def call_function(function_call_part, verbose=False):
	
function_call_part is a types.FunctionCall that most importantly has:

◦A .name property (the name of the function, a string)
◦A .args property (a dictionary of named arguments to the function)
If verbose is specified, print the function name and args:
print(f"Calling function: {function_call_part.name}({function_call_part.args})")

Otherwise, just print the name:
print(f" - Calling function: {function_call_part.name}")

2. Based on the name, actually call the function and capture the result

◦Be sure to manually add the "working_directory" argument to the dictionary of keyword arguments, because the LLM doesn't control that one. 
 The working directory should be ./calculator.
◦The syntax to pass a dictionary into a function using keyword arguments is some_function(**some_args)

I used a dictionary of function name (string) -> function to accomplish this.

3.If the function name is invalid, return a types.Content that explains the error:
return types.Content(
    role="tool",
    parts=[
        types.Part.from_function_response(
            name=function_name,
            response={"error": f"Unknown function: {function_name}"},
        )
    ],
)

4.Return types.Content with a from_function_response describing the result of the function call:
return types.Content(
    role="tool",
    parts=[
        types.Part.from_function_response(
            name=function_name,
            response={"result": function_result},
        )
    ],
)

5. Back where you handle the response from the model generate_content, instead of simply printing the name of the function the LLM decides to call, use call_function:
◦The types.Content that we return from call_function should have a .parts[0].function_response.response within.
◦If it doesn't, raise a fatal exception of some sort.
◦If it does, append the function call's response (.parts[0]) to a list -- we'll use this later.
◦If verbose was set, print the result of the function call like this:
print(f"-> {function_call_result.parts[0].function_response.response}")

6.Test your program. You should now be able to execute each function given a prompt that asks for it. 
Try some different prompts and use the --verbose flag to make sure all the functions work.
◦List the directory contents
◦Get a file's contents
◦Write file contents (don't overwrite anything important, maybe create a new file)
◦Execute the calculator app's tests (tests.py)

We aren't passing the function call results back to the LLM just yet.

===========================================================================================================================================	
Agents
So we've got some function calling working, but it's not fair to call our program an "agent" yet for one simple reason:

It has no feedback loop.

A key part of an "Agent", as defined by AI-influencer-hype-bros, is that it can continuously use its tools to iterate on its own results. So we're going to build two things:

A loop that will call the LLM over and over
A list of messages in the "conversation". It will look something like this:
User: "Please fix the bug in the calculator"
Model: "I want to call get_files_info..."
Tool: "Here's the result of get_files_info..."
Model: "I want to call get_file_content..."
Tool: "Here's the result of get_file_content..."
Model: "I want to call run_python_file..."
Tool: "Here's the result of run_python_file..."
Model: "I want to call write_file..."
Tool: "Here's the result of write_file..."
Model: "I want to call run_python_file..."
Tool: "Here's the result of run_python_file..."
Model: "I fixed the bug and then ran the calculator to ensure it's working."
This is a pretty big step, take your time!

Assignment:

--In generate_content, handle the results of any possible tool use:
1.This might already be happening, but make sure that with each call to client.models.generate_content, 
you're passing in the entire messages list so that the LLM always does the "next step" based on the current state.

2.After calling client's generate_content method, check the .candidates property of the response. 
It's a list of response variations (usually just one). It contains the equivalent of "I want to call get_files_info...", so we need to add it to our conversation. Iterate over each candidate and add its .content to your messages list.

3.After you have the responses from each function call, use the types.
Content function to convert the list of responses into a message with a role of user and append it into your messages.

--Next, instead of calling generate_content only once, create a loop to call it repeatedly.
1.Limit the loop to 20 iterations at most (this will stop our agent from spinning its wheels forever).

2.Use a try-except block and handle any errors accordingly.

3.After each call of generate_content, determine if the model is finished. 
It's finished only if no candidate contains a function call and response.text is non-empty. 
In that case, print the final response and break out of the loop; otherwise continue the loop.

4.Otherwise, iterate again (unless max iterations was reached, of course).

--Test your code . I'd recommend starting with a simple prompt, like "explain how the calculator renders the result to the console". 
